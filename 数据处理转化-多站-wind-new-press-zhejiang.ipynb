{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06ae4a3c-c1c3-4285-8713-bbab7f495c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修改后的第一阶段：收集高度、读取数据并确定各文件实际时间范围...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点文件读取与时间范围分析: 100%|██████████████████████████████████████████████████████| 18/18 [11:27<00:00, 38.20s/it]\n",
      "C:\\Users\\49294\\AppData\\Local\\Temp\\ipykernel_22112\\100714637.py:149: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  common_target_time_6min_freq = pd.date_range(start=common_period_start_aligned, end=common_period_end_aligned, freq='6T')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第一阶段完成：共处理了 18 个包含有效时间数据的文件。\n",
      "\n",
      "所有有效文件的最小公共时间段为:\n",
      "  开始 (Latest Start): 2024-07-29 09:30:00\n",
      "  结束 (Earliest End): 2025-07-21 23:54:00\n",
      "基于公共时间段和6分钟频率生成的目标时间序列:\n",
      "  开始: 2024-07-29 09:30:00\n",
      "  结束: 2025-07-21 23:54:00\n",
      "  点数: 85825\n",
      "\n",
      "收集到的全局唯一高度值 (共 308 个): [60, 100, 120, 150, 180, 240, 270, 300, 340, 360, 390, 420, 480, 510, 540, 580, 600, 630, 720, 750, 820, 840, 870, 960, 990, 1060, 1080, 1110, 1200, 1230, 1300, 1320, 1350, 1440, 1470, 1540, 1560, 1590, 1680, 1710, 1780, 1800, 1830, 1920, 1950, 2020, 2040, 2070, 2160, 2190, 2260, 2280, 2310, 2400, 2430, 2500, 2520, 2550, 2640, 2670, 2740, 2760, 2790, 2880, 2910, 2980, 3000, 3030, 3120, 3150, 3220, 3240, 3270, 3360, 3390, 3460, 3480, 3510, 3600, 3630, 3700, 3720, 3750, 3840, 3870, 3940, 3960, 3990, 4000, 4080, 4110, 4180, 4200, 4230, 4240, 4320, 4350, 4420, 4440, 4470, 4480, 4560, 4590, 4660, 4680, 4710, 4720, 4800, 4830, 4900, 4920, 4950, 4960, 5040, 5070, 5140, 5160, 5190, 5200, 5280, 5310, 5380, 5400, 5430, 5440, 5520, 5550, 5620, 5640, 5670, 5680, 5760, 5790, 5860, 5910, 5920, 6030, 6100, 6150, 6160, 6270, 6340, 6390, 6400, 6510, 6580, 6630, 6640, 6750, 6820, 6870, 6880, 6990, 7060, 7110, 7120, 7230, 7300, 7350, 7360, 7470, 7540, 7590, 7600, 7710, 7780, 7830, 7840, 7950, 8020, 8070, 8080, 8190, 8260, 8310, 8320, 8430, 8500, 8550, 8560, 8670, 8740, 8790, 8800, 8910, 8980, 9030, 9040, 9150, 9220, 9270, 9280, 9390, 9460, 9510, 9520, 9630, 9700, 9750, 9760, 9870, 9940, 9990, 10000, 10110, 10180, 10230, 10240, 10350, 10420, 10470, 10480, 10590, 10660, 10710, 10720, 10830, 10900, 10950, 10960, 11070, 11140, 11190, 11310, 11380, 11430, 11550, 11620, 11670, 11790, 11860, 11910, 12030, 12100, 12150, 12270, 12340, 12390, 12510, 12580, 12630, 12750, 12820, 12870, 12990, 13060, 13110, 13230, 13300, 13350, 13470, 13540, 13590, 13710, 13780, 13830, 13950, 14020, 14070, 14190, 14260, 14310, 14430, 14500, 14550, 14670, 14740, 14790, 14910, 14980, 15030, 15150, 15220, 15270, 15390, 15460, 15510, 15630, 15700, 15750, 15940, 15990, 16180, 16230, 16420, 16470, 16660, 16710, 16900, 16950, 17140, 17190, 17380, 17430, 17620, 17670, 17860, 17910, 18100, 18150, 18340, 18390, 18580, 18630, 18820, 18870, 19060, 19110]\n",
      "\n",
      "修改后的第二阶段：基于公共时间段（6分钟频率）处理数据并生成NetCDF文件...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:   0%|                                                                 | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  对站点 58448 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58448 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:   6%|███▏                                                     | 1/18 [00:34<09:43, 34.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58448_pressure_coords_filled.nc\n",
      "  对站点 58450 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58450 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  11%|██████▎                                                  | 2/18 [01:03<08:21, 31.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58450_pressure_coords_filled.nc\n",
      "  对站点 58452 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58452 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  17%|█████████▌                                               | 3/18 [01:29<07:15, 29.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58452_pressure_coords_filled.nc\n",
      "  对站点 58459 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58459 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  22%|████████████▋                                            | 4/18 [01:55<06:24, 27.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58459_pressure_coords_filled.nc\n",
      "  对站点 58467 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58467 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  28%|███████████████▊                                         | 5/18 [02:19<05:41, 26.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58467_pressure_coords_filled.nc\n",
      "  对站点 58468 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58468 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  33%|███████████████████                                      | 6/18 [02:42<05:05, 25.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58468_pressure_coords_filled.nc\n",
      "  对站点 58537 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58537 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  39%|██████████████████████▏                                  | 7/18 [03:07<04:37, 25.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58537_pressure_coords_filled.nc\n",
      "  对站点 58542 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58542 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  44%|█████████████████████████▎                               | 8/18 [03:28<03:59, 23.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58542_pressure_coords_filled.nc\n",
      "  对站点 58543 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58543 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  50%|████████████████████████████▌                            | 9/18 [03:54<03:41, 24.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58543_pressure_coords_filled.nc\n",
      "  对站点 58548 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58548 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  56%|███████████████████████████████                         | 10/18 [04:19<03:16, 24.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58548_pressure_coords_filled.nc\n",
      "  对站点 58553 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58553 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  61%|██████████████████████████████████▏                     | 11/18 [04:41<02:46, 23.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58553_pressure_coords_filled.nc\n",
      "  对站点 58557 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58557 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  67%|█████████████████████████████████████▎                  | 12/18 [05:05<02:22, 23.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58557_pressure_coords_filled.nc\n",
      "  对站点 58565 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58565 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  72%|████████████████████████████████████████▍               | 13/18 [05:29<01:59, 23.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58565_pressure_coords_filled.nc\n",
      "  对站点 58566 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58566 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  78%|███████████████████████████████████████████▌            | 14/18 [05:52<01:34, 23.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58566_pressure_coords_filled.nc\n",
      "  对站点 58643 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58643 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  83%|██████████████████████████████████████████████▋         | 15/18 [06:16<01:10, 23.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58643_pressure_coords_filled.nc\n",
      "  对站点 58646 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58646 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  89%|█████████████████████████████████████████████████▊      | 16/18 [06:39<00:46, 23.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58646_pressure_coords_filled.nc\n",
      "  对站点 58751 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58751 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成:  94%|████████████████████████████████████████████████████▉   | 17/18 [07:01<00:23, 23.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58751_pressure_coords_filled.nc\n",
      "  对站点 58755 进行高度到气压层的插值到 [1000  925  850  700  600  500  400] hPa...\n",
      "  站点 58755 数据已插值到气压层。新的维度 'PRESS' (hPa): [1000  925  850  700  600  500  400]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "站点数据处理与NetCDF生成: 100%|████████████████████████████████████████████████████████| 18/18 [07:25<00:00, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  成功保存NetCDF文件: D:/wind/zhejiang_all_less/58755_pressure_coords_filled.nc\n",
      "\n",
      "所有站点个体NetCDF文件处理完毕，保存在: D:/wind/zhejiang_all_less/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import traceback # 用于打印详细错误信息\n",
    "\n",
    "# 定义根目录\n",
    "root_dir = 'D:/wind/zhejiang_all_less/'\n",
    "\n",
    "# 给定的站点 ID 列表\n",
    "station_ids_list = [ # Renamed to avoid conflict with loop variable\n",
    "    \"58448\", \"58450\",  \"58452\", \"58459\",\n",
    "    \"58467\", \"58468\", \"58537\", \"58542\", \"58543\", \"58548\",\n",
    "    \"58553\",\"58557\",\"58565\",\"58566\",\"58643\",\"58646\",\"58751\",\n",
    "    \"58755\"\n",
    "]\n",
    "\n",
    "# --- 新增：定义物理常数和目标气压层 ---\n",
    "P0_STANDARD_HPA = 1013.25  # 标准海平面气压 (hPa)\n",
    "H_SCALE_METERS = 8000      # 标高 (m)，用于气压高度公式\n",
    "TARGET_PRESSURES_HPA = np.array([1000, 925, 850, 700, 600, 500, 400]) # 目标气压层 (hPa)\n",
    "\n",
    "# --- 新增：气压-高度转换函数 ---\n",
    "def height_to_pressure(height, P0=P0_STANDARD_HPA, H_scale=H_SCALE_METERS):\n",
    "    \"\"\"将高度 (米) 转换为气压 (hPa)\"\"\"\n",
    "    pressure = P0 * np.exp(-np.array(height, dtype=float) / H_scale)\n",
    "    return pressure\n",
    "\n",
    "def pressure_to_height(pressure, P0=P0_STANDARD_HPA, H_scale=H_SCALE_METERS):\n",
    "    \"\"\"将气压 (hPa) 转换为高度 (米)\"\"\"\n",
    "    pressure_arr = np.array(pressure, dtype=float)\n",
    "    # 对气压值进行裁剪以避免log计算错误，并处理P > P0的情况 (高度为负)\n",
    "    pressure_clipped = np.clip(pressure_arr, 1e-5, P0 * 2) # 允许一定范围的外插\n",
    "    height = -H_scale * np.log(pressure_clipped / P0)\n",
    "    return height\n",
    "# --- 新增结束 ---\n",
    "\n",
    "all_heights_set = set()\n",
    "dataframes_raw = {} # 存储每个站点的原始 DataFrame\n",
    "station_actual_time_ranges = {} # 存储每个站点实际的 (min_time, max_time)\n",
    "\n",
    "print(\"修改后的第一阶段：收集高度、读取数据并确定各文件实际时间范围...\")\n",
    "global_latest_start_time = None\n",
    "global_earliest_end_time = None\n",
    "valid_files_count_for_time_range = 0\n",
    "\n",
    "for station_name_loopvar in tqdm(station_ids_list, desc=\"站点文件读取与时间范围分析\"):\n",
    "    station_path = os.path.join(root_dir, station_name_loopvar)\n",
    "    if os.path.isdir(station_path) and os.listdir(station_path):\n",
    "        filename = os.path.join(station_path, '20230601_20250721.txt')\n",
    "        if os.path.isfile(filename):\n",
    "            datas = []\n",
    "            try:\n",
    "                with open(filename, 'r', encoding='utf-8') as files:\n",
    "                    try:\n",
    "                        next(files) # 跳过表头第一行\n",
    "                        next(files) # 跳过表头第二行\n",
    "                    except StopIteration:\n",
    "                        print(f\"警告：文件 {filename} 为空或表头不足两行，跳过。\")\n",
    "                        continue\n",
    "                    for line_number, line in enumerate(files, start=3):\n",
    "                        try:\n",
    "                            line = line.strip()\n",
    "                            if not line: continue\n",
    "                            parts = re.split(r'\\s+', line)\n",
    "                            if len(parts) < 12: continue # 确保行数据足够\n",
    "                            datetime_str = parts[1] + ' ' + parts[2]\n",
    "                            direction_h = float(parts[5])\n",
    "                            speed_h = float(parts[6])\n",
    "                            height = int(parts[7])\n",
    "                            speed_v = float(parts[8])\n",
    "                            reliability_h = int(parts[9])\n",
    "                            reliability_v = int(parts[10])\n",
    "                            all_heights_set.add(height)\n",
    "                            datas.append({\n",
    "                                'Datetime_str': datetime_str,\n",
    "                                'DIRECTION_H': direction_h, 'SPEED_H': speed_h, 'HEIGHT': height,\n",
    "                                'SPEED_V': speed_v, 'RELIABILITY_H': reliability_h, 'RELIABILITY_V': reliability_v,\n",
    "                            })\n",
    "                        except (ValueError, IndexError) : continue # 跳过格式错误的行\n",
    "                        except Exception as e_line:\n",
    "                            print(f\"错误：处理文件 {filename} 第 {line_number} 行时发生未知错误: {e_line}，跳过此行。\")\n",
    "            except Exception as e_file:\n",
    "                print(f\"错误：无法打开或读取文件 {filename}: {e_file}\")\n",
    "                continue\n",
    "            if datas:\n",
    "                df_temp = pd.DataFrame(datas)\n",
    "                try:\n",
    "                    df_temp['Datetime'] = pd.to_datetime(df_temp['Datetime_str'], errors='coerce')\n",
    "                    df_temp.dropna(subset=['Datetime'], inplace=True) # 移除无法解析的日期\n",
    "                    df_temp.drop(columns=['Datetime_str'], inplace=True)\n",
    "                    if not df_temp.empty:\n",
    "                        dataframes_raw[station_name_loopvar] = df_temp\n",
    "                        current_min_time = df_temp['Datetime'].min()\n",
    "                        current_max_time = df_temp['Datetime'].max()\n",
    "                        station_actual_time_ranges[station_name_loopvar] = (current_min_time, current_max_time)\n",
    "                        if global_latest_start_time is None:\n",
    "                            global_latest_start_time = current_min_time\n",
    "                            global_earliest_end_time = current_max_time\n",
    "                        else:\n",
    "                            global_latest_start_time = max(global_latest_start_time, current_min_time)\n",
    "                            global_earliest_end_time = min(global_earliest_end_time, current_max_time)\n",
    "                        valid_files_count_for_time_range += 1\n",
    "                    else:\n",
    "                        print(f\"信息：站点 {station_name_loopvar} 文件 {filename} 解析日期后无有效数据。\")\n",
    "                except Exception as e_df:\n",
    "                    print(f\"错误：转换站点 {station_name_loopvar} 的 Datetime 列或计算其时间范围失败: {e_df}\")\n",
    "            else:\n",
    "                print(f\"信息：站点 {station_name_loopvar} 文件 {filename} 未读取到有效数据行。\")\n",
    "        else:\n",
    "            print(f\"警告：文件 {filename} 不存在。\")\n",
    "    else:\n",
    "        print(f\"警告：目录 {station_path} 不存在或为空。\")\n",
    "\n",
    "if not dataframes_raw:\n",
    "    print(\"错误：未能从任何文件中读取到有效数据，程序终止。\")\n",
    "    exit()\n",
    "if not all_heights_set:\n",
    "    print(\"错误：未能从任何文件中收集到高度信息，程序终止。\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n第一阶段完成：共处理了 {valid_files_count_for_time_range} 个包含有效时间数据的文件。\")\n",
    "\n",
    "if global_latest_start_time is None or global_earliest_end_time is None or global_latest_start_time >= global_earliest_end_time:\n",
    "    print(\"错误：未能找到所有有效文件的公共时间段，或公共时间段无效。程序终止。\")\n",
    "    print(f\"  计算出的最晚开始时间 (global_latest_start_time): {global_latest_start_time}\")\n",
    "    print(f\"  计算出的最早结束时间 (global_earliest_end_time): {global_earliest_end_time}\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\n所有有效文件的最小公共时间段为:\")\n",
    "print(f\"  开始 (Latest Start): {global_latest_start_time}\")\n",
    "print(f\"  结束 (Earliest End): {global_earliest_end_time}\")\n",
    "\n",
    "try:\n",
    "    common_period_start_aligned = global_latest_start_time.ceil('6min')\n",
    "    common_period_end_aligned = global_earliest_end_time.floor('6min')\n",
    "except AttributeError as e_align:\n",
    "    print(f\"错误：对齐公共时间段边界时出错。可能是时间戳对象类型不正确。{e_align}\")\n",
    "    print(f\"  global_latest_start_time: {global_latest_start_time} (type: {type(global_latest_start_time)})\")\n",
    "    print(f\"  global_earliest_end_time: {global_earliest_end_time} (type: {type(global_earliest_end_time)})\")\n",
    "    exit()\n",
    "\n",
    "if common_period_start_aligned >= common_period_end_aligned:\n",
    "    print(f\"错误：对齐后的公共时间段无效。开始: {common_period_start_aligned}, 结束: {common_period_end_aligned}。程序终止。\")\n",
    "    exit()\n",
    "\n",
    "common_target_time_6min_freq = pd.date_range(start=common_period_start_aligned, end=common_period_end_aligned, freq='6T')\n",
    "if common_target_time_6min_freq.empty:\n",
    "    print(f\"错误：基于公共时间段生成的6分钟目标时间序列为空。对齐后开始: {common_period_start_aligned}, 结束: {common_period_end_aligned}。程序终止。\")\n",
    "    exit()\n",
    "\n",
    "print(f\"基于公共时间段和6分钟频率生成的目标时间序列:\")\n",
    "print(f\"  开始: {common_target_time_6min_freq.min()}\")\n",
    "print(f\"  结束: {common_target_time_6min_freq.max()}\")\n",
    "print(f\"  点数: {len(common_target_time_6min_freq)}\")\n",
    "\n",
    "global_unique_heights = sorted(list(all_heights_set))\n",
    "print(f\"\\n收集到的全局唯一高度值 (共 {len(global_unique_heights)} 个): {global_unique_heights}\")\n",
    "\n",
    "print(\"\\n修改后的第二阶段：基于公共时间段（6分钟频率）处理数据并生成NetCDF文件...\")\n",
    "output_nc_dir_final = root_dir # 输出目录保持不变\n",
    "os.makedirs(output_nc_dir_final, exist_ok=True)\n",
    "\n",
    "for station_name_loopvar_p2 in tqdm(station_ids_list, desc=\"站点数据处理与NetCDF生成\"):\n",
    "    if station_name_loopvar_p2 not in dataframes_raw:\n",
    "        print(f\"信息：站点 {station_name_loopvar_p2} 无原始数据，跳过。\")\n",
    "        continue\n",
    "    df_station_current_original = dataframes_raw[station_name_loopvar_p2]\n",
    "    if df_station_current_original.empty:\n",
    "        print(f\"信息：站点 {station_name_loopvar_p2} 原始数据为空，跳过。\")\n",
    "        continue\n",
    "\n",
    "    df_station_in_common_period = df_station_current_original[\n",
    "        (df_station_current_original['Datetime'] >= common_period_start_aligned) &\n",
    "        (df_station_current_original['Datetime'] <= common_period_end_aligned)\n",
    "    ].copy()\n",
    "\n",
    "    if df_station_in_common_period.empty:\n",
    "        print(f\"信息：站点 {station_name_loopvar_p2} 在公共时间段 {common_period_start_aligned} 到 {common_period_end_aligned} 内无数据，跳过。\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df_station_no_duplicates_p2 = df_station_in_common_period.drop_duplicates(subset=['Datetime', 'HEIGHT'], keep='first').copy()\n",
    "        dfs_pivot_p2 = df_station_no_duplicates_p2.pivot_table(\n",
    "            index='Datetime',\n",
    "            columns='HEIGHT',\n",
    "            values=['DIRECTION_H', 'RELIABILITY_H', 'RELIABILITY_V', 'SPEED_H', 'SPEED_V']\n",
    "        )\n",
    "    except Exception as e_pivot:\n",
    "        print(f\"错误：为站点 {station_name_loopvar_p2} 创建 pivot table 失败: {e_pivot}\")\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "    if dfs_pivot_p2.empty:\n",
    "        print(f\"信息：站点 {station_name_loopvar_p2} 的 pivot_table 为空，跳过。\")\n",
    "        continue\n",
    "\n",
    "    dfs_pivot_p2.replace(9999, np.nan, inplace=True) # 将9999替换为NaN\n",
    "\n",
    "    if isinstance(dfs_pivot_p2.columns, pd.MultiIndex) and len(dfs_pivot_p2.columns.levels) > 1:\n",
    "        heights_in_pivot = dfs_pivot_p2.columns.levels[1].unique().astype(int)\n",
    "    else: # pivot table 列结构非预期，可能因为只有一个高度层或变量\n",
    "        print(f\"警告: 站点 {station_name_loopvar_p2} 的 pivot_table 列结构非预期，尝试使用全局高度。\")\n",
    "        # 尝试从单层索引中提取高度（如果适用）\n",
    "        try:\n",
    "            # 如果列名是元组 (var, height)，但不是MultiIndex，尝试解析\n",
    "            parsed_heights = set()\n",
    "            for col in dfs_pivot_p2.columns:\n",
    "                if isinstance(col, tuple) and len(col) == 2:\n",
    "                    parsed_heights.add(col[1])\n",
    "            if parsed_heights:\n",
    "                heights_in_pivot = sorted(list(parsed_heights))\n",
    "            else: # 否则回退到全局高度\n",
    "                heights_in_pivot = global_unique_heights\n",
    "        except:\n",
    "             heights_in_pivot = global_unique_heights\n",
    "\n",
    "\n",
    "    data_vars_xr = {}\n",
    "    expected_vars_list = ['DIRECTION_H', 'RELIABILITY_H', 'RELIABILITY_V', 'SPEED_H', 'SPEED_V']\n",
    "    datetime_coords_for_xr = dfs_pivot_p2.index\n",
    "\n",
    "    for var_name_xr in expected_vars_list:\n",
    "        try:\n",
    "            # 构建 (len(Datetime), len(heights_in_pivot)) 的数组\n",
    "            var_data_values_xr = np.full((len(datetime_coords_for_xr), len(heights_in_pivot)), np.nan)\n",
    "            for i, h_val in enumerate(heights_in_pivot):\n",
    "                if (var_name_xr, h_val) in dfs_pivot_p2.columns:\n",
    "                    var_data_values_xr[:, i] = dfs_pivot_p2[(var_name_xr, h_val)].values\n",
    "            data_vars_xr[var_name_xr] = (['Datetime', 'HEIGHT_pivot'], var_data_values_xr)\n",
    "        except KeyError: # 如果某个变量在pivot表中完全不存在\n",
    "             data_vars_xr[var_name_xr] = (['Datetime', 'HEIGHT_pivot'], np.full((len(datetime_coords_for_xr), len(heights_in_pivot)), np.nan))\n",
    "        except Exception as e_prepare_var:\n",
    "            print(f\"错误：为站点 {station_name_loopvar_p2} 准备变量 {var_name_xr} 时出错: {e_prepare_var}\")\n",
    "            data_vars_xr[var_name_xr] = (['Datetime', 'HEIGHT_pivot'], np.full((len(datetime_coords_for_xr), len(heights_in_pivot)), np.nan))\n",
    "\n",
    "\n",
    "    dss_initial = xr.Dataset(\n",
    "        data_vars_xr,\n",
    "        coords={\n",
    "            'Datetime': ('Datetime', datetime_coords_for_xr.values),\n",
    "            'HEIGHT_pivot': ('HEIGHT_pivot', heights_in_pivot)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    dss_reindexed_h_p2 = dss_initial.reindex({'HEIGHT_pivot': global_unique_heights}).rename({'HEIGHT_pivot': 'HEIGHT'})\n",
    "    dss_filled_orig_time_p2 = dss_reindexed_h_p2.ffill(dim='Datetime', limit=None).bfill(dim='Datetime', limit=None)\n",
    "\n",
    "    dss_interp_height_p2 = dss_filled_orig_time_p2.copy(deep=True)\n",
    "    for var_xr in list(dss_interp_height_p2.data_vars):\n",
    "        dss_interp_height_p2[var_xr] = dss_interp_height_p2[var_xr].interpolate_na(dim='HEIGHT', method='linear', limit=None)\n",
    "        dss_interp_height_p2[var_xr] = dss_interp_height_p2[var_xr].ffill(dim='HEIGHT').bfill(dim='HEIGHT')\n",
    "\n",
    "    dss_with_uvw_p2 = dss_interp_height_p2.copy(deep=True)\n",
    "    if 'DIRECTION_H' in dss_interp_height_p2 and 'SPEED_H' in dss_interp_height_p2 and 'SPEED_V' in dss_interp_height_p2:\n",
    "        direction_rad_p2 = np.deg2rad(dss_interp_height_p2['DIRECTION_H'])\n",
    "        speed_h_data_p2 = dss_interp_height_p2['SPEED_H']\n",
    "        U_wind_p2 = -speed_h_data_p2 * np.sin(direction_rad_p2)\n",
    "        V_wind_p2 = -speed_h_data_p2 * np.cos(direction_rad_p2)\n",
    "        dss_with_uvw_p2 = dss_interp_height_p2.assign(\n",
    "            U=(['Datetime', 'HEIGHT'], U_wind_p2.data),\n",
    "            V=(['Datetime', 'HEIGHT'], V_wind_p2.data),\n",
    "            W=(['Datetime', 'HEIGHT'], dss_interp_height_p2['SPEED_V'].data) # SPEED_V is already W\n",
    "        )\n",
    "    else:\n",
    "        print(f\"  警告: 站点 {station_name_loopvar_p2} 缺少计算U,V,W所需变量，U,V,W将不被计算。\")\n",
    "\n",
    "    target_time_index_for_reindex = pd.DatetimeIndex(common_target_time_6min_freq)\n",
    "    dss_time_aligned_p2 = dss_with_uvw_p2.reindex({'Datetime': target_time_index_for_reindex})\n",
    "    dss_time_interp_p2 = dss_time_aligned_p2.interpolate_na(dim='Datetime', method='linear', limit=None)\n",
    "\n",
    "    dss_filled_along_time_p2 = dss_time_interp_p2.ffill(dim='Datetime', limit=None).bfill(dim='Datetime', limit=None)\n",
    "    dss_final_filled_p2 = dss_filled_along_time_p2.ffill(dim='HEIGHT', limit=None).bfill(dim='HEIGHT', limit=None)\n",
    "\n",
    "    # ----- 修改开始: 高度到气压层的插值 -----\n",
    "    dss_to_save = None # Initialize dss_to_save\n",
    "    print(f\"  对站点 {station_name_loopvar_p2} 进行高度到气压层的插值到 {TARGET_PRESSURES_HPA} hPa...\")\n",
    "    if 'HEIGHT' not in dss_final_filled_p2.coords or not hasattr(dss_final_filled_p2['HEIGHT'], 'values') or len(dss_final_filled_p2['HEIGHT'].values) == 0:\n",
    "        print(f\"  警告: 站点 {station_name_loopvar_p2} 的 dss_final_filled_p2 中缺少有效 'HEIGHT' 坐标，跳过气压插值。\")\n",
    "        dss_to_save = dss_final_filled_p2 # 保存未经气压插值的数据\n",
    "    else:\n",
    "        try:\n",
    "            current_target_pressures_hpa = TARGET_PRESSURES_HPA\n",
    "\n",
    "            # 1. 计算目标气压层对应的高度值\n",
    "            # target_heights_m 的顺序将对应 current_target_pressures_hpa 的顺序\n",
    "            target_heights_m = pressure_to_height(current_target_pressures_hpa, P0=P0_STANDARD_HPA, H_scale=H_SCALE_METERS)\n",
    "\n",
    "            if np.any(~np.isfinite(target_heights_m)):\n",
    "                print(f\"  警告: 站点 {station_name_loopvar_p2} 计算目标高度时出现非有限值 for pressures {current_target_pressures_hpa}. Heights: {target_heights_m}. 跳过气压插值。\")\n",
    "                dss_to_save = dss_final_filled_p2\n",
    "            else:\n",
    "                # 2. 将数据集插值到这些新的高度值\n",
    "                # dss_final_filled_p2.HEIGHT 应该是单调递增的\n",
    "                dss_interp_at_equiv_heights = dss_final_filled_p2.interp(\n",
    "                    HEIGHT=target_heights_m, # interp的HEIGHT参数应为目标高度值\n",
    "                    method=\"linear\",\n",
    "                    kwargs={\"fill_value\": np.nan} # 超出范围的值用NaN填充\n",
    "                )\n",
    "                # 此时，dss_interp_at_equiv_heights 的 'HEIGHT' 坐标的值是 target_heights_m\n",
    "\n",
    "                # 3. 重命名维度并将坐标值替换为目标气压值\n",
    "                dss_renamed_dim = dss_interp_at_equiv_heights.rename({'HEIGHT': 'PRESS'})\n",
    "                \n",
    "                # 将PRESS坐标的值赋为我们期望的目标气压层\n",
    "                dss_on_pressure = dss_renamed_dim.assign_coords(\n",
    "                    PRESS=('PRESS', current_target_pressures_hpa)\n",
    "                )\n",
    "\n",
    "                # 4. 填充插值后可能产生的NaN值\n",
    "                for var_name_final in dss_on_pressure.data_vars:\n",
    "                    # 沿新的 PRESS 维度填充\n",
    "                    dss_on_pressure[var_name_final] = dss_on_pressure[var_name_final].ffill(dim='PRESS').bfill(dim='PRESS')\n",
    "                    # （可选）再次沿时间维度填充，以防万一\n",
    "                    dss_on_pressure[var_name_final] = dss_on_pressure[var_name_final].ffill(dim='Datetime').bfill(dim='Datetime')\n",
    "                \n",
    "                dss_to_save = dss_on_pressure\n",
    "                print(f\"  站点 {station_name_loopvar_p2} 数据已插值到气压层。新的维度 'PRESS' (hPa): {dss_to_save.PRESS.values}\")\n",
    "\n",
    "        except Exception as e_interp_press:\n",
    "            print(f\"错误：站点 {station_name_loopvar_p2} 在插值到气压层时失败: {e_interp_press}\")\n",
    "            traceback.print_exc()\n",
    "            print(f\"  将保存未经气压插值的数据。\")\n",
    "            dss_to_save = dss_final_filled_p2 # Fallback\n",
    "    # ----- 修改结束 -----\n",
    "\n",
    "    # 更新输出文件名并保存 dss_to_save\n",
    "    if dss_to_save is None or dss_to_save.equals(dss_final_filled_p2) :\n",
    "        filename_nc_output = os.path.join(output_nc_dir_final, f\"{station_name_loopvar_p2}_height_coords_filled.nc\")\n",
    "    else: # 插值成功\n",
    "        filename_nc_output = os.path.join(output_nc_dir_final, f\"{station_name_loopvar_p2}_pressure_coords_filled.nc\")\n",
    "\n",
    "    try:\n",
    "        if 'Datetime' in dss_to_save.coords:\n",
    "             dss_to_save['Datetime'].attrs['long_name'] = 'time'\n",
    "             dss_to_save['Datetime'].attrs['standard_name'] = 'time'\n",
    "        \n",
    "        # 为坐标添加属性\n",
    "        if 'PRESS' in dss_to_save.coords:\n",
    "            dss_to_save['PRESS'].attrs['units'] = 'hPa'\n",
    "            dss_to_save['PRESS'].attrs['long_name'] = 'Pressure Level'\n",
    "            dss_to_save['PRESS'].attrs['standard_name'] = 'air_pressure'\n",
    "            dss_to_save['PRESS'].attrs['positive'] = 'down' # 气压向下增加\n",
    "        elif 'HEIGHT' in dss_to_save.coords: # 如果仍然是高度坐标\n",
    "             dss_to_save['HEIGHT'].attrs['units'] = 'm'\n",
    "             dss_to_save['HEIGHT'].attrs['long_name'] = 'Height above ground level' # 假设是AGL\n",
    "             dss_to_save['HEIGHT'].attrs['standard_name'] = 'height'\n",
    "             dss_to_save['HEIGHT'].attrs['positive'] = 'up'   # 高度向上增加\n",
    "\n",
    "        encoding_dict = {var: {'zlib': True, 'complevel': 5} for var in dss_to_save.data_vars}\n",
    "        dss_to_save.to_netcdf(filename_nc_output, encoding=encoding_dict)\n",
    "        print(f\"  成功保存NetCDF文件: {filename_nc_output}\")\n",
    "    except Exception as e_save:\n",
    "        print(f\"错误：保存NetCDF文件 {filename_nc_output} 失败: {e_save}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\n所有站点个体NetCDF文件处理完毕，保存在: {output_nc_dir_final}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af834eb2-5809-4d7d-80e2-292586e02aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Third Stage: Merging Station NetCDF Files (with PRESS dimension) ---\n",
      "Reading individual .nc files from: D:/wind/zhejiang_all_less/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging station files:   0%|                                                                    | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Inferred common Datetime coordinate from D:/wind/zhejiang_all_less/58448_pressure_coords_filled.nc (length 85825)\n",
      "  Inferred common PRESS coordinate from D:/wind/zhejiang_all_less/58448_pressure_coords_filled.nc (length 7, values: [1000  925  850  700  600]...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Merging station files: 100%|███████████████████████████████████████████████████████████| 18/18 [00:03<00:00,  5.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully prepared 18 station datasets for merging.\n",
      "Using station IDs: [58448, 58450, 58452, 58459, 58467, 58468, 58537, 58542, 58543, 58548, 58553, 58557, 58565, 58566, 58643, 58646, 58751, 58755]\n",
      "\n",
      "Concatenating datasets along 'station' dimension...\n",
      "Concatenation complete.\n",
      "\n",
      "Transposing data variables to ('Datetime', 'station', 'PRESS')...\n",
      "Transpose complete.\n",
      "\n",
      "Verifying coordinates...\n",
      "  Final 'Datetime' coordinate verified (length 85825).\n",
      "  Final 'PRESS' coordinate verified (length 7, values: [1000  925  850  700  600]...).\n",
      "  Final 'station' coordinate verified (dtype: int32, values: [58448 58450 58452 58459 58467]...).\n",
      "\n",
      "Converting float64 data variables to float32 for memory efficiency...\n",
      "  Converting variable 'DIRECTION_H' from float64 to float32.\n",
      "  Converting variable 'RELIABILITY_H' from float64 to float32.\n",
      "  Converting variable 'RELIABILITY_V' from float64 to float32.\n",
      "  Converting variable 'SPEED_H' from float64 to float32.\n",
      "  Converting variable 'SPEED_V' from float64 to float32.\n",
      "  Converting variable 'U' from float64 to float32.\n",
      "  Converting variable 'V' from float64 to float32.\n",
      "  Converting variable 'W' from float64 to float32.\n",
      "\n",
      "--- Final Dataset Structure Check (after dtype conversion) ---\n",
      "<xarray.Dataset> Size: 347MB\n",
      "Dimensions:        (Datetime: 85825, station: 18, PRESS: 7)\n",
      "Coordinates:\n",
      "  * station        (station) int32 72B 58448 58450 58452 ... 58646 58751 58755\n",
      "  * Datetime       (Datetime) datetime64[ns] 687kB 2024-07-29T09:30:00 ... 20...\n",
      "  * PRESS          (PRESS) int32 28B 1000 925 850 700 600 500 400\n",
      "Data variables:\n",
      "    DIRECTION_H    (Datetime, station, PRESS) float32 43MB 110.6 120.7 ... 165.5\n",
      "    RELIABILITY_H  (Datetime, station, PRESS) float32 43MB 86.0 91.82 ... 89.0\n",
      "    RELIABILITY_V  (Datetime, station, PRESS) float32 43MB 89.0 89.0 ... 94.43\n",
      "    SPEED_H        (Datetime, station, PRESS) float32 43MB 6.4 5.735 ... 7.643\n",
      "    SPEED_V        (Datetime, station, PRESS) float32 43MB 0.2 0.3825 ... 1.0\n",
      "    U              (Datetime, station, PRESS) float32 43MB -5.991 ... -1.912\n",
      "    V              (Datetime, station, PRESS) float32 43MB 2.252 2.929 ... 7.4\n",
      "    W              (Datetime, station, PRESS) float32 43MB 0.2 0.3825 ... 1.0\n",
      "  Found 8 data variables.\n",
      "    Variable 'DIRECTION_H': Dimensions ('Datetime', 'station', 'PRESS'), Shape (85825, 18, 7), Dtype float32\n",
      "    Variable 'RELIABILITY_H': Dimensions ('Datetime', 'station', 'PRESS'), Shape (85825, 18, 7), Dtype float32\n",
      "    Variable 'RELIABILITY_V': Dimensions ('Datetime', 'station', 'PRESS'), Shape (85825, 18, 7), Dtype float32\n",
      "    Variable 'SPEED_H': Dimensions ('Datetime', 'station', 'PRESS'), Shape (85825, 18, 7), Dtype float32\n",
      "    Variable 'SPEED_V': Dimensions ('Datetime', 'station', 'PRESS'), Shape (85825, 18, 7), Dtype float32\n",
      "    Variable 'U': Dimensions ('Datetime', 'station', 'PRESS'), Shape (85825, 18, 7), Dtype float32\n",
      "    Variable 'V': Dimensions ('Datetime', 'station', 'PRESS'), Shape (85825, 18, 7), Dtype float32\n",
      "    Variable 'W': Dimensions ('Datetime', 'station', 'PRESS'), Shape (85825, 18, 7), Dtype float32\n",
      "  All data variables have the correct dimension order.\n",
      "\n",
      "Saving final combined dataset to: D:/wind/zhejiang_all_less/merged_stations_6min_common_period_float32_no_nan_press.nc\n",
      "Successfully saved: D:/wind/zhejiang_all_less/merged_stations_6min_common_period_float32_no_nan_press.nc\n",
      "Verify with: ncdump -h D:/wind/zhejiang_all_less/merged_stations_6min_common_period_float32_no_nan_press.nc\n",
      "\n",
      "--- Third Stage Processing Complete ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "\n",
    "# --- Configuration for Third Stage ---\n",
    "root_dir = 'D:/wind/zhejiang_all_less/'\n",
    "station_ids_list_stage3 =  [\n",
    "    \"58448\", \"58450\",  \"58452\", \"58459\",\n",
    "    \"58467\", \"58468\", \"58537\", \"58542\", \"58543\", \"58548\",\n",
    "    \"58553\",\"58557\",\"58565\",\"58566\",\"58643\",\"58646\",\"58751\",\n",
    "    \"58755\"\n",
    "]\n",
    "station_coords_int_stage3 = [int(sid) for sid in station_ids_list_stage3]\n",
    "input_nc_dir_stage3 = root_dir # Individual station files are in the root_dir\n",
    "\n",
    "# --- MODIFICATION: Reflect new vertical dimension ---\n",
    "inferred_common_time_coord = None\n",
    "inferred_global_press_coord = None # Changed from inferred_global_height_coord\n",
    "\n",
    "print(\"\\n--- Third Stage: Merging Station NetCDF Files (with PRESS dimension) ---\")\n",
    "datasets_to_combine = []\n",
    "valid_station_coords_for_concat = []\n",
    "\n",
    "print(f\"Reading individual .nc files from: {input_nc_dir_stage3}\")\n",
    "for station_id_str, station_id_int in tqdm(zip(station_ids_list_stage3, station_coords_int_stage3), total=len(station_ids_list_stage3), desc=\"Merging station files\"):\n",
    "    # --- MODIFICATION: Update input file name to match Stage 2 output for pressure data ---\n",
    "    file_path = os.path.join(input_nc_dir_stage3, f\"{station_id_str}_pressure_coords_filled.nc\")\n",
    "    # Fallback: if the above doesn't exist, maybe user has a different naming or it's height data\n",
    "    # You might want to add more sophisticated logic here if files can have either HEIGHT or PRESS\n",
    "    # and you want to merge them differently or handle height-based files.\n",
    "    # For now, we strictly look for pressure-interpolated files.\n",
    "    if not os.path.exists(file_path):\n",
    "        # If you also want to process files named \"_no_nan_press.nc\" or \"_height_coords_filled.nc\",\n",
    "        # you can add more checks here.\n",
    "        # Example:\n",
    "        # fallback_file_path_user_naming = os.path.join(input_nc_dir_stage3, f\"{station_id_str}_no_nan_press.nc\")\n",
    "        # if os.path.exists(fallback_file_path_user_naming):\n",
    "        #     file_path = fallback_file_path_user_naming\n",
    "        # else:\n",
    "        print(f\"INFO: Expected pressure file {file_path} does not exist. Skipping station {station_id_str}.\")\n",
    "        continue\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with xr.open_dataset(file_path) as ds_station:\n",
    "                # --- MODIFICATION: Check for 'PRESS' coordinate ---\n",
    "                if 'Datetime' not in ds_station.coords or 'PRESS' not in ds_station.coords:\n",
    "                    print(f\"  WARNING: File {file_path} is missing 'Datetime' or 'PRESS' coordinates. Skipping.\")\n",
    "                    continue\n",
    "                if not pd.api.types.is_datetime64_any_dtype(ds_station['Datetime'].dtype):\n",
    "                    print(f\"  WARNING: File {file_path} 'Datetime' coordinate is not datetime64. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                if inferred_common_time_coord is None:\n",
    "                    inferred_common_time_coord = ds_station['Datetime'].copy(deep=True)\n",
    "                    print(f\"  Inferred common Datetime coordinate from {file_path} (length {len(inferred_common_time_coord)})\")\n",
    "                # --- MODIFICATION: Infer 'PRESS' coordinate ---\n",
    "                if inferred_global_press_coord is None:\n",
    "                    inferred_global_press_coord = ds_station['PRESS'].copy(deep=True)\n",
    "                    print(f\"  Inferred common PRESS coordinate from {file_path} (length {len(inferred_global_press_coord)}, values: {inferred_global_press_coord.values[:5]}...)\")\n",
    "\n",
    "                if not ds_station['Datetime'].equals(inferred_common_time_coord):\n",
    "                    print(f\"  CRITICAL WARNING: Datetime coordinate in {file_path} MISMATCHES inferred common Datetime. Skipping file.\")\n",
    "                    continue\n",
    "                # --- MODIFICATION: Verify 'PRESS' coordinate ---\n",
    "                if not ds_station['PRESS'].equals(inferred_global_press_coord):\n",
    "                    print(f\"  CRITICAL WARNING: PRESS coordinate in {file_path} MISMATCHES inferred common PRESS. Skipping file.\")\n",
    "                    print(f\"    File PRESS: {ds_station['PRESS'].values}\")\n",
    "                    print(f\"    Expected PRESS: {inferred_global_press_coord.values}\")\n",
    "                    continue\n",
    "\n",
    "                ds_station_expanded = ds_station.expand_dims(station=[station_id_int])\n",
    "                datasets_to_combine.append(ds_station_expanded)\n",
    "                valid_station_coords_for_concat.append(station_id_int)\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: Failed to read or process file {file_path}: {e}\")\n",
    "            traceback.print_exc()\n",
    "    # This 'else' block is now covered by the 'continue' above if file_path doesn't exist.\n",
    "    # else:\n",
    "    #     print(f\"INFO: File {file_path} does not exist. Skipping.\")\n",
    "\n",
    "\n",
    "if not datasets_to_combine:\n",
    "    print(\"ERROR: No NetCDF files were successfully read and prepared for merging. Terminating.\")\n",
    "    exit()\n",
    "# --- MODIFICATION: Check inferred_global_press_coord ---\n",
    "elif inferred_common_time_coord is None or inferred_global_press_coord is None:\n",
    "    print(\"ERROR: Could not infer common Datetime or PRESS coordinates from the input files. Terminating.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"\\nSuccessfully prepared {len(datasets_to_combine)} station datasets for merging.\")\n",
    "print(f\"Using station IDs: {valid_station_coords_for_concat}\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nConcatenating datasets along 'station' dimension...\")\n",
    "    combined_ds = xr.concat(\n",
    "        datasets_to_combine, dim='station', data_vars='all',\n",
    "        coords='all', join='override', compat='no_conflicts' # Presumes coordinates are identical where they should be\n",
    "    )\n",
    "    print(\"Concatenation complete.\")\n",
    "\n",
    "    # --- MODIFICATION: Update target_dims_order to use 'PRESS' ---\n",
    "    print(\"\\nTransposing data variables to ('Datetime', 'station', 'PRESS')...\")\n",
    "    target_dims_order = ('Datetime', 'station', 'PRESS')\n",
    "    missing_dims = [d for d in target_dims_order if d not in combined_ds.dims]\n",
    "    if missing_dims:\n",
    "        print(f\"ERROR: Cannot transpose. Dataset is missing dimensions: {missing_dims}. Available: {list(combined_ds.dims.keys())}\")\n",
    "        exit()\n",
    "    \n",
    "    # Ensure the station coordinate is correctly assigned if it was part of the concat\n",
    "    if 'station' not in combined_ds.coords:\n",
    "        combined_ds = combined_ds.assign_coords(station=valid_station_coords_for_concat)\n",
    "\n",
    "    combined_ds_transposed = combined_ds.transpose(*target_dims_order)\n",
    "    print(\"Transpose complete.\")\n",
    "\n",
    "    print(\"\\nVerifying coordinates...\")\n",
    "    if 'Datetime' not in combined_ds_transposed.coords or not combined_ds_transposed['Datetime'].equals(inferred_common_time_coord):\n",
    "        print(\"ERROR: Final 'Datetime' coordinate is missing or does not match inferred common time.\")\n",
    "        exit()\n",
    "    print(f\"  Final 'Datetime' coordinate verified (length {len(combined_ds_transposed.Datetime)}).\")\n",
    "    # --- MODIFICATION: Verify 'PRESS' coordinate ---\n",
    "    if 'PRESS' not in combined_ds_transposed.coords or not combined_ds_transposed['PRESS'].equals(inferred_global_press_coord):\n",
    "        print(\"ERROR: Final 'PRESS' coordinate is missing or does not match inferred common press levels.\")\n",
    "        exit()\n",
    "    print(f\"  Final 'PRESS' coordinate verified (length {len(combined_ds_transposed.PRESS)}, values: {combined_ds_transposed.PRESS.values[:5]}...).\")\n",
    "\n",
    "    if 'station' not in combined_ds_transposed.coords:\n",
    "        print(\"ERROR: Final 'station' coordinate is missing.\")\n",
    "        combined_ds_transposed = combined_ds_transposed.assign_coords(station=valid_station_coords_for_concat)\n",
    "        if 'station' not in combined_ds_transposed.coords: exit()\n",
    "    # Ensure station coordinate is integer\n",
    "    if combined_ds_transposed['station'].dtype != np.int64 and combined_ds_transposed['station'].dtype != np.int32:\n",
    "        print(f\"  Converting 'station' coordinate from {combined_ds_transposed['station'].dtype} to int...\")\n",
    "        combined_ds_transposed['station'] = combined_ds_transposed['station'].astype(int)\n",
    "    print(f\"  Final 'station' coordinate verified (dtype: {combined_ds_transposed['station'].dtype}, values: {combined_ds_transposed['station'].values[:5]}...).\")\n",
    "\n",
    "\n",
    "    print(\"\\nConverting float64 data variables to float32 for memory efficiency...\")\n",
    "    for var_name in list(combined_ds_transposed.data_vars.keys()):\n",
    "        if combined_ds_transposed[var_name].dtype == np.float64:\n",
    "            print(f\"  Converting variable '{var_name}' from float64 to float32.\")\n",
    "            combined_ds_transposed[var_name] = combined_ds_transposed[var_name].astype(np.float32)\n",
    "\n",
    "    print(\"\\n--- Final Dataset Structure Check (after dtype conversion) ---\")\n",
    "    print(combined_ds_transposed)\n",
    "    all_vars_dims_correct = True\n",
    "    if combined_ds_transposed.data_vars:\n",
    "        print(f\"  Found {len(combined_ds_transposed.data_vars)} data variables.\")\n",
    "        for var_name, data_array in combined_ds_transposed.data_vars.items():\n",
    "            print(f\"    Variable '{var_name}': Dimensions {data_array.dims}, Shape {data_array.shape}, Dtype {data_array.dtype}\")\n",
    "            if data_array.dims != target_dims_order:\n",
    "                print(f\"      WARNING: Variable '{var_name}' has incorrect dimension order! Expected {target_dims_order}, Got {data_array.dims}\")\n",
    "                all_vars_dims_correct = False\n",
    "        if all_vars_dims_correct: print(\"  All data variables have the correct dimension order.\")\n",
    "        else: print(\"  ERROR: Not all data variables have the correct dimension order. Please review.\")\n",
    "    else:\n",
    "        print(\"ERROR: No data variables found in the final combined dataset!\")\n",
    "        exit()\n",
    "\n",
    "    # --- MODIFICATION: Update output filename (user's desired name) ---\n",
    "    output_filename = \"merged_stations_6min_common_period_float32_no_nan_press.nc\"\n",
    "    output_combined_file_path = os.path.join(root_dir, output_filename)\n",
    "    print(f\"\\nSaving final combined dataset to: {output_combined_file_path}\")\n",
    "    try:\n",
    "        encoding = {var: {'zlib': True, 'complevel': 5} for var in combined_ds_transposed.data_vars}\n",
    "        # Attributes for Datetime and PRESS should be carried over from individual files\n",
    "        # if they were set correctly in stage 2. We can double-check/set standard ones.\n",
    "        if 'Datetime' in combined_ds_transposed.coords:\n",
    "            combined_ds_transposed['Datetime'].attrs['long_name'] = 'time'\n",
    "            combined_ds_transposed['Datetime'].attrs['standard_name'] = 'time'\n",
    "        if 'PRESS' in combined_ds_transposed.coords: # Set attributes if not already present or to ensure consistency\n",
    "            combined_ds_transposed['PRESS'].attrs.setdefault('units', 'hPa')\n",
    "            combined_ds_transposed['PRESS'].attrs.setdefault('long_name', 'Pressure Level')\n",
    "            combined_ds_transposed['PRESS'].attrs.setdefault('standard_name', 'air_pressure')\n",
    "            combined_ds_transposed['PRESS'].attrs.setdefault('positive', 'down') # Standard for pressure coordinates\n",
    "\n",
    "\n",
    "        combined_ds_transposed.to_netcdf(\n",
    "            output_combined_file_path,\n",
    "            encoding=encoding,\n",
    "            unlimited_dims=['Datetime']\n",
    "        )\n",
    "        print(f\"Successfully saved: {output_combined_file_path}\")\n",
    "        print(f\"Verify with: ncdump -h {output_combined_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Failed to save the final NetCDF file: {e}\")\n",
    "        traceback.print_exc()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the third stage processing: {e}\")\n",
    "    traceback.print_exc()\n",
    "print(\"\\n--- Third Stage Processing Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37d2633-ecdc-4207-8117-8529008645b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

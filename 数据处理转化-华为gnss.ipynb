{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66416bd9-634f-4048-a769-c17568372213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理根目录: E:\\huawei_gnss\n",
      "\n",
      "--- 正在处理文件夹: zhejiang-20250514-6min ---\n",
      "  找到 885 个 CSV 文件。\n",
      "  成功合并 1506443 行数据，包含 885 个站点。\n",
      "\n",
      "--- 正在处理文件夹: zhejiang-20250522-6min ---\n",
      "  找到 1392 个 CSV 文件。\n",
      "  成功合并 2258021 行数据，包含 1392 个站点。\n",
      "\n",
      "--- 正在处理文件夹: zhejiang-20250530-6min ---\n",
      "  找到 935 个 CSV 文件。\n",
      "  成功合并 1598436 行数据，包含 935 个站点。\n",
      "\n",
      "--- 正在处理文件夹: zhejiang-20250607-6min ---\n",
      "  找到 1545 个 CSV 文件。\n",
      "  成功合并 2394346 行数据，包含 1545 个站点。\n",
      "\n",
      "--- 正在处理文件夹: zhejiang-20250616-6min ---\n",
      "  找到 1545 个 CSV 文件。\n",
      "  成功合并 2198632 行数据，包含 1545 个站点。\n",
      "\n",
      "--- 正在处理文件夹: zhejiang-20250623-6min ---\n",
      "  找到 1502 个 CSV 文件。\n",
      "  成功合并 2177916 行数据，包含 1502 个站点。\n",
      "\n",
      "--- 正在处理文件夹: zhejiang-20250629-6min ---\n",
      "  找到 1542 个 CSV 文件。\n",
      "  成功合并 2075315 行数据，包含 1542 个站点。\n",
      "\n",
      "--- 正在处理文件夹: zhejiang-20250705-6min ---\n",
      "  找到 1440 个 CSV 文件。\n",
      "  成功合并 2038759 行数据，包含 1440 个站点。\n",
      "\n",
      "--- 正在处理文件夹: zhejiang-20250711-6min ---\n",
      "  找到 1440 个 CSV 文件。\n",
      "  成功合并 2236752 行数据，包含 1440 个站点。\n",
      "\n",
      "--- 正在处理文件夹: zhejiang-20250720-6min ---\n",
      "  找到 1428 个 CSV 文件。\n",
      "  成功合并 1947882 行数据，包含 1428 个站点。\n",
      "\n",
      "--- 数据整合 ---\n",
      "在所有 10 个文件夹中找到 566 个公共站点。\n",
      "正在合并所有文件夹的数据...\n",
      "过滤数据，只保留 566 个公共站点...\n",
      "正在将时间戳重采样到规整的6分钟间隔...\n",
      "对同一时间间隔内的多个数据点进行聚合（计算平均值）...\n",
      "正在将聚合后的数据转换为 time x station 的二维格式...\n",
      "正在创建 xarray.DataArray...\n",
      "正在保存数据到: E:/gnss_ztd_combined_robust_6min.nc\n",
      "\n",
      "处理成功！NetCDF 文件已生成。\n",
      "最终数据维度信息:\n",
      "<xarray.DataArray 'ztd' (time: 18080, station: 566)> Size: 41MB\n",
      "array([[  nan,   nan,   nan, ...,   nan,   nan,   nan],\n",
      "       [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n",
      "       [  nan,   nan,   nan, ...,   nan,   nan,   nan],\n",
      "       ...,\n",
      "       [  nan,   nan,   nan, ..., 2.672, 2.667, 2.681],\n",
      "       [  nan,   nan,   nan, ..., 2.671, 2.666, 2.681],\n",
      "       [  nan,   nan,   nan, ...,   nan,   nan,   nan]], dtype=float32)\n",
      "Coordinates:\n",
      "  * time     (time) datetime64[ns] 145kB 2025-05-06T07:12:00 ... 2025-07-21T0...\n",
      "  * station  (station) <U9 20kB 'H11548254' 'H11558032' ... 'H999926' 'H999941'\n",
      "Attributes:\n",
      "    long_name:    Zenith Total Delay\n",
      "    units:        mm\n",
      "    description:  GNSS Zenith Total Delay data, resampled to 6-minute interva...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from functools import reduce\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from functools import reduce\n",
    "\n",
    "def process_gnss_data_from_csv_robust(root_path: str, output_filename: str):\n",
    "    \"\"\"\n",
    "    处理指定路径下的GNSS数据（CSV格式），整合为xarray.DataArray并输出为NetCDF文件。\n",
    "    该版本对缺失数据更具鲁棒性，并增加了对不规整时间的重采样功能。\n",
    "\n",
    "    Args:\n",
    "        root_path (str): 包含各省份数据文件夹的根目录。\n",
    "        output_filename (str): 输出的 .nc 文件名。\n",
    "    \"\"\"\n",
    "    print(f\"开始处理根目录: {root_path}\")\n",
    "    \n",
    "    # 用于存储每个日期文件夹处理后的数据和站点列表\n",
    "    all_folder_dataframes = []\n",
    "    all_folder_stations = []\n",
    "\n",
    "    # 1. 遍历根目录下的所有子文件夹\n",
    "    for folder_name in os.listdir(root_path):\n",
    "        folder_path = os.path.join(root_path, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- 正在处理文件夹: {folder_name} ---\")\n",
    "        \n",
    "        csv_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.csv')]\n",
    "        if not csv_files:\n",
    "            print(\"  未找到 CSV 文件，跳过此文件夹。\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  找到 {len(csv_files)} 个 CSV 文件。\")\n",
    "\n",
    "        daily_dfs = []\n",
    "        stations_in_this_folder = set()\n",
    "\n",
    "        for file in csv_files:\n",
    "            try:\n",
    "                file_path = os.path.join(folder_path, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                if not {'siteID', ' datetime', ' ztd'}.issubset(df.columns):\n",
    "                    print(f\"  文件 {file} 缺少必要的列，跳过。\")\n",
    "                    continue\n",
    "                \n",
    "                df['datetime'] = pd.to_datetime(df[' datetime'])\n",
    "                df['siteID'] = df['siteID'].astype(str)\n",
    "                df['ztd'] = pd.to_numeric(df[' ztd'], errors='coerce')\n",
    "                \n",
    "                daily_dfs.append(df)\n",
    "                stations_in_this_folder.add(df['siteID'].iloc[0])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  读取或处理文件 {file} 时出错: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not daily_dfs:\n",
    "            print(\"  未能成功读取任何文件，跳过此文件夹。\")\n",
    "            continue\n",
    "\n",
    "        daily_combined_df = pd.concat(daily_dfs, ignore_index=True)\n",
    "        print(f\"  成功合并 {len(daily_combined_df)} 行数据，包含 {len(stations_in_this_folder)} 个站点。\")\n",
    "        \n",
    "        all_folder_dataframes.append(daily_combined_df)\n",
    "        all_folder_stations.append(stations_in_this_folder)\n",
    "\n",
    "    if not all_folder_dataframes:\n",
    "        print(\"\\n处理完成，但未找到任何有效数据。程序退出。\")\n",
    "        return\n",
    "\n",
    "    common_stations = reduce(lambda a, b: a.intersection(b), all_folder_stations)\n",
    "    print(f\"\\n--- 数据整合 ---\")\n",
    "    print(f\"在所有 {len(all_folder_stations)} 个文件夹中找到 {len(common_stations)} 个公共站点。\")\n",
    "\n",
    "    if not common_stations:\n",
    "        print(\"所有文件夹之间没有公共站点，无法创建数据集。程序退出。\")\n",
    "        return\n",
    "\n",
    "    print(\"正在合并所有文件夹的数据...\")\n",
    "    final_combined_df = pd.concat(all_folder_dataframes, ignore_index=True)\n",
    "\n",
    "    print(f\"过滤数据，只保留 {len(common_stations)} 个公共站点...\")\n",
    "    final_combined_df = final_combined_df[final_combined_df['siteID'].isin(common_stations)]\n",
    "    final_combined_df.dropna(subset=['datetime', 'ztd'], inplace=True)\n",
    "\n",
    "    if final_combined_df.empty:\n",
    "        print(\"最终数据为空，可能是公共站点没有有效值。程序退出。\")\n",
    "        return\n",
    "\n",
    "    # --- 从这里开始是关键修改 ---\n",
    "\n",
    "    # 5. 【新增】时间重采样 (Resampling) 和聚合\n",
    "    print(\"正在将时间戳重采样到规整的6分钟间隔...\")\n",
    "    # 使用 .dt.round('6T') 将时间四舍五入到最近的6分钟点。\n",
    "    # '6T' 是 pandas 的频率字符串，代表 6 分钟。\n",
    "    # 例如, '2025-05-06T07:11:38' 会被四舍五入到 '2025-05-06T07:12:00'\n",
    "    final_combined_df['datetime'] = final_combined_df['datetime'].dt.round('6T')\n",
    "\n",
    "    print(\"对同一时间间隔内的多个数据点进行聚合（计算平均值）...\")\n",
    "    # 由于多个原始时间点可能被归入同一个6分钟点，我们需要对它们进行聚合。\n",
    "    # 按站点(siteID)和新的规整时间(datetime)分组，然后计算 ztd 的平均值。\n",
    "    # .reset_index() 会将分组结果重新变回 DataFrame。\n",
    "    final_aggregated_df = final_combined_df.groupby(['datetime', 'siteID'])['ztd'].mean().reset_index()\n",
    "\n",
    "    # 6. 【修改】将聚合后的数据转换为二维表格 (time x station)\n",
    "    print(\"正在将聚合后的数据转换为 time x station 的二维格式...\")\n",
    "    # 注意：这里要使用我们刚刚创建的聚合后的 DataFrame: final_aggregated_df\n",
    "    pivot_df = final_aggregated_df.pivot_table(\n",
    "        index='datetime', \n",
    "        columns='siteID', \n",
    "        values='ztd'\n",
    "    )\n",
    "    \n",
    "    # --- 修改结束 ---\n",
    "\n",
    "    # 对时间和站点进行排序，使数据更规整\n",
    "    pivot_df = pivot_df.sort_index()\n",
    "    pivot_df = pivot_df.reindex(sorted(pivot_df.columns), axis=1)\n",
    "\n",
    "    # 7. 创建 xarray.DataArray\n",
    "    print(\"正在创建 xarray.DataArray...\")\n",
    "    da = xr.DataArray(\n",
    "        data=pivot_df.values.astype(np.float32),\n",
    "        dims=(\"time\", \"station\"),\n",
    "        coords={\n",
    "            \"time\": pivot_df.index.values,\n",
    "            \"station\": pivot_df.columns.values.astype(str)\n",
    "        },\n",
    "        name=\"ztd\"\n",
    "    )\n",
    "    \n",
    "    da.attrs['long_name'] = 'Zenith Total Delay'\n",
    "    da.attrs['units'] = 'mm'\n",
    "    da.attrs['description'] = 'GNSS Zenith Total Delay data, resampled to 6-minute intervals. Missing values are filled with NaN.'\n",
    "\n",
    "    # 8. 保存为 .nc 文件\n",
    "    try:\n",
    "        encoding = {\n",
    "            'ztd': {'_FillValue': np.nan, 'dtype': 'float32'},\n",
    "            'time': {'units': 'seconds since 1970-01-01 00:00:00', 'dtype': 'double'}\n",
    "        }\n",
    "        print(f\"正在保存数据到: {output_filename}\")\n",
    "        da.to_netcdf(output_filename, encoding=encoding)\n",
    "        print(\"\\n处理成功！NetCDF 文件已生成。\")\n",
    "        print(\"最终数据维度信息:\")\n",
    "        print(da)\n",
    "    except Exception as e:\n",
    "        print(f\"保存为 NetCDF 文件时出错: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # --- 请修改以下参数 ---\n",
    "    # 1. 您的数据根目录路径\n",
    "    data_root_path = r'E:\\huawei_gnss' \n",
    "    \n",
    "    # 2. 您希望输出的 .nc 文件名\n",
    "    output_nc_file = 'E:/gnss_ztd_combined_robust_6min.nc'\n",
    "    # --- 参数修改结束 ---\n",
    "\n",
    "    if not os.path.exists(data_root_path):\n",
    "        print(f\"错误：路径 '{data_root_path}' 不存在。请检查您的路径设置。\")\n",
    "    else:\n",
    "        process_gnss_data_from_csv_robust(data_root_path, output_nc_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5476e72f-e8d0-4afd-87f4-ba2a6a47f4cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
